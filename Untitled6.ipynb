{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVcZ5qmtDzSseW3ERM7Du9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivastherapati/neural-hw3/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eMpJaLnnr5y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# 1. Load the MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the data to [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Flatten the images (28x28 pixels -> 784 features)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# 2. Define a fully connected (Dense) autoencoder\n",
        "latent_dim = 32  # Latent dimension size (can be modified to 16 or 64)\n",
        "\n",
        "# Encoder\n",
        "input_img = layers.Input(shape=(784,))\n",
        "encoded = layers.Dense(latent_dim, activation='relu')(input_img)\n",
        "\n",
        "# Decoder\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "\n",
        "# 3. Compile and train the autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "# 4. Plot original vs reconstructed images after training\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n = 10  # Number of images to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original images\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Display reconstructed images\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# 5. Modify the latent dimension size (e.g., 16, 64) and analyze how it affects the quality of reconstruction\n",
        "# To experiment, change the latent_dim value above and observe the reconstruction quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# 1. Load the MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the data to [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Flatten the images (28x28 pixels -> 784 features)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# 2. Add Gaussian noise to the input images\n",
        "def add_noise(images, noise_factor=0.5):\n",
        "    noisy_images = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
        "    noisy_images = np.clip(noisy_images, 0., 1.)  # Ensure values stay within [0, 1]\n",
        "    return noisy_images\n",
        "\n",
        "# Add noise to training and test images\n",
        "x_train_noisy = add_noise(x_train)\n",
        "x_test_noisy = add_noise(x_test)\n",
        "\n",
        "# 3. Define a fully connected (Dense) autoencoder (Denoising Autoencoder)\n",
        "latent_dim = 32  # Latent dimension size\n",
        "\n",
        "# Encoder\n",
        "input_img = layers.Input(shape=(784,))\n",
        "encoded = layers.Dense(latent_dim, activation='relu')(input_img)\n",
        "\n",
        "# Decoder\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# Denoising Autoencoder Model\n",
        "denoising_autoencoder = models.Model(input_img, decoded)\n",
        "\n",
        "# 4. Compile and train the denoising autoencoder\n",
        "denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model on noisy images to reconstruct clean images\n",
        "denoising_autoencoder.fit(x_train_noisy, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# 5. Plot noisy vs reconstructed images after training\n",
        "decoded_imgs = denoising_autoencoder.predict(x_test_noisy)\n",
        "\n",
        "n = 10  # Number of images to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display noisy images\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Display original images\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Display reconstructed images\n",
        "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gk-jGbwgnyn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import random\n",
        "import string\n",
        "\n",
        "# 1. Load a text dataset (Example: Shakespeare Sonnet)\n",
        "# For this, we can use a sample of Shakespeare's sonnets\n",
        "text = \"\"\"\n",
        "    When, in disgrace with fortune and men's eyes,\n",
        "    I all alone beweep my outcast state,\n",
        "    And trouble deaf heaven with my bootless cries,\n",
        "    And look upon myself and curse my fate,\n",
        "    Wishing me like to one more rich in hope,\n",
        "    Featur'd like him, like him with friends possess'd,\n",
        "    Desiring this man's art and that man's scope,\n",
        "    With what I most enjoy contented least;\n",
        "    Yet in these thoughts myself almost despising,\n",
        "    Haply I think on thee, and then my state,\n",
        "    Like to the lark at break of day arising\n",
        "    From sullen earth, sings hymns at heaven's gate;\n",
        "    For thy sweet love remember'd such wealth brings\n",
        "    That then I scorn to change my state with kings.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Preprocess the text\n",
        "# Create a mapping from characters to indices\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {char: index for index, char in enumerate(chars)}\n",
        "index_to_char = {index: char for index, char in enumerate(chars)}\n",
        "\n",
        "# Convert text into integers (sequence of characters as integers)\n",
        "text_as_int = [char_to_index[char] for char in text]\n",
        "\n",
        "# Define sequence length (number of characters in each input sequence)\n",
        "seq_length = 100\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "# Prepare input and output sequences\n",
        "for i in range(0, len(text_as_int) - seq_length, 1):\n",
        "    x.append(text_as_int[i:i + seq_length])\n",
        "    y.append(text_as_int[i + seq_length])\n",
        "\n",
        "# Reshape inputs and normalize\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(chars))\n",
        "\n",
        "# 3. Define the RNN model (LSTM)\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(seq_length,)),\n",
        "    layers.Embedding(len(chars), 128),\n",
        "    layers.LSTM(256, return_sequences=False),\n",
        "    layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile and train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(x, y, batch_size=64, epochs=20)\n",
        "\n",
        "# 5. Generate text by sampling characters one at a time\n",
        "def generate_text(model, start_string, length=500, temperature=1.0):\n",
        "    # Convert the start string to integers\n",
        "    input_sequence = [char_to_index[char] for char in start_string]\n",
        "\n",
        "    # Reshape the input to match model's expected input shape\n",
        "    input_sequence = np.expand_dims(input_sequence, axis=0)\n",
        "\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Predict the next character probabilities\n",
        "        predictions = model.predict(input_sequence)[0]\n",
        "\n",
        "        # Temperature scaling (adjusts the randomness of predictions)\n",
        "        predictions = predictions / temperature\n",
        "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))  # Softmax\n",
        "\n",
        "        # Sample from the distribution (choose character with highest probability)\n",
        "        next_index = np.random.choice(len(chars), p=predictions)\n",
        "        next_char = index_to_char[next_index]\n",
        "\n",
        "        # Add the next character to the generated text\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update the input sequence to include the new character\n",
        "        input_sequence = np.roll(input_sequence, -1, axis=1)\n",
        "        input_sequence[0, -1] = next_index\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# 6. Generate new text with a starting string\n",
        "start_string = \"When, in disgrace with\"\n",
        "generated_text = generate_text(model, start_string, length=500, temperature=0.7)\n",
        "\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "id": "AfuSEvOWn5-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "# 2. Preprocess the text data: Tokenize and pad sequences\n",
        "# Pad sequences to make them of uniform length\n",
        "maxlen = 500  # Maximum length of review sequence\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# 3. Build the LSTM-based model\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=10000, output_dim=128, input_length=maxlen),\n",
        "    layers.LSTM(64, return_sequences=False),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# 5. Evaluate the model: Predict on test data\n",
        "y_pred = (model.predict(x_test) > 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
        "\n",
        "# 6. Confusion Matrix and Classification Report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n",
        "\n",
        "# Plotting confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HWIAlTeSoAax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}